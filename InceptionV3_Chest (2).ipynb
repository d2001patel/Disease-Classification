{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "InceptionV3_Chest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFjZxmFn3hmJ",
        "outputId": "7abe436c-b52c-4be4-d8e9-93d029dac8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive/COVID_0'\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AE66GAm2HDOHof1uhNoyum8hh1yVBDSYERQ7JX05cVZLVpdOjESbcI\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/COVID_0\n",
            " Covid2.ipynb\t        inceptionv3chest_00.hdf5   NORMAL\n",
            " Data\t\t        inceptionv3chest_0.hdf5    Test\n",
            " inceptionchest_00.h5   inceptionv3chest_1.hdf5    vggchest2_0.h5\n",
            " inceptionchest_0.h5    inceptionv3chest_2.hdf5   'VGG chest.ipynb'\n",
            " inceptionchest_1.h5    InceptionV3_Chest.ipynb    vggweightschest0.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mArgDd393_54",
        "outputId": "27aa6a82-9aea-4b28-9687-bb0cce139ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from builtins import range, input\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, AveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHFmAOJ9ajx1"
      },
      "source": [
        "### Define Few Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h6epi3D6WGQ"
      },
      "source": [
        "#define size to which images are to be resized\n",
        "IMAGE_SIZE = [224, 224] \n",
        "\n",
        "# training config:\n",
        "epochs = 500\n",
        "batch_size = 32\n",
        "\n",
        "#define paths\n",
        "covid_path = 'Data/Chest_COVID'\n",
        "noncovid_path = 'Data/Chest_NonCOVID'\n",
        "\n",
        "# Use glob to grab images from path .jpg or jpeg\n",
        "covid_files = glob(covid_path + '/*')\n",
        "noncovid_files = glob(noncovid_path + '/*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpCwxG6Vcuu9"
      },
      "source": [
        "### Fetch Images and Class Labels from Files (This might take a while)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DmylTfn7XND"
      },
      "source": [
        "# Preparing Labels\n",
        "covid_labels = []\n",
        "noncovid_labels = []\n",
        "\n",
        "covid_images=[]\n",
        "noncovid_images=[]\n",
        "\n",
        "import cv2 \n",
        "\n",
        "for i in range(len(covid_files)):\n",
        "  image = cv2.imread(covid_files[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  covid_images.append(image)\n",
        "  covid_labels.append('Chest_COVID')\n",
        "for i in range(len(noncovid_files)):\n",
        "  image = cv2.imread(noncovid_files[i])\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  noncovid_images.append(image)\n",
        "  noncovid_labels.append('Chest_NonCOVID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puWJQfMQcc0U",
        "outputId": "6042cd1e-2347-4915-8f21-862262c85f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(covid_labels)\n",
        "len(noncovid_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4oSc4ake4N4"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Model takes images in the form of array of pixels. Hence convert into array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtlBn63m7jbT"
      },
      "source": [
        "# normalize to interval of [0,1]\n",
        "covid_images = np.array(covid_images) / 255\n",
        "noncovid_images = np.array(noncovid_images) / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXltydCxfxTo"
      },
      "source": [
        "### **Train Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJPR0OjX-IFv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# split into training and testing\n",
        "covid_x_train, covid_x_test, covid_y_train, covid_y_test = train_test_split(\n",
        "    covid_images, covid_labels, test_size=0.2)\n",
        "noncovid_x_train, noncovid_x_test, noncovid_y_train, noncovid_y_test = train_test_split(\n",
        "    noncovid_images, noncovid_labels, test_size=0.2)\n",
        "\n",
        "\n",
        "X_train = np.concatenate((noncovid_x_train, covid_x_train), axis=0)\n",
        "X_test = np.concatenate((noncovid_x_test, covid_x_test), axis=0)\n",
        "y_train = np.concatenate((noncovid_y_train, covid_y_train), axis=0)\n",
        "y_test = np.concatenate((noncovid_y_test, covid_y_test), axis=0)\n",
        "\n",
        "# make labels into categories - either 0 or 1\n",
        "y_train = LabelBinarizer().fit_transform(y_train)\n",
        "y_train = to_categorical(y_train)\n",
        "\n",
        "y_test = LabelBinarizer().fit_transform(y_test)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T3wkds0N5hK"
      },
      "source": [
        "### **Compile model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47Amw8c-iTl",
        "outputId": "747a76d5-7cdb-4dfe-df5a-b012358adf70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "inception = InceptionV3(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "outputs = inception.output\n",
        "outputs = Flatten(name=\"flatten\")(outputs)\n",
        "outputs = Dropout(0.5)(outputs)\n",
        "outputs = Dense(2, activation=\"softmax\")(outputs)\n",
        "\n",
        "model = Model(inputs=inception.input, outputs=outputs)\n",
        "\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "        loss='categorical_crossentropy', \n",
        "        optimizer='adam', \n",
        "        metrics=['accuracy']\n",
        ")\n",
        "\n",
        "train_aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6z8EtY_--Ar",
        "outputId": "041d6915-5689-469f-d753-a77987bbe1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 111, 111, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 111, 111, 32) 96          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 111, 111, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 109, 109, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 109, 109, 32) 96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 109, 109, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 109, 109, 64) 18432       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 109, 109, 64) 192         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 109, 109, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 54, 54, 64)   0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 54, 54, 80)   5120        max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 54, 54, 80)   240         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 54, 54, 80)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 52, 52, 192)  138240      activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 52, 52, 192)  576         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 52, 52, 192)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 25, 25, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 25, 25, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 25, 25, 48)   9216        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 25, 25, 96)   55296       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 25, 25, 48)   144         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 25, 25, 96)   288         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 25, 25, 48)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 25, 25, 96)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 25, 25, 192)  0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 25, 25, 64)   12288       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 25, 25, 64)   76800       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 25, 25, 96)   82944       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 25, 25, 32)   6144        average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 25, 25, 64)   192         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 25, 25, 64)   192         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 25, 25, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 25, 25, 32)   96          conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 25, 25, 64)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 25, 25, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 25, 25, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 25, 25, 32)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 25, 25, 256)  0           activation_5[0][0]               \n",
            "                                                                 activation_7[0][0]               \n",
            "                                                                 activation_10[0][0]              \n",
            "                                                                 activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 25, 25, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 25, 25, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 25, 25, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 25, 25, 96)   55296       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 25, 25, 48)   144         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 25, 25, 96)   288         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 25, 25, 48)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 25, 25, 96)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 25, 25, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 25, 25, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 25, 25, 64)   76800       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 25, 25, 96)   82944       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 25, 25, 64)   16384       average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 25, 25, 64)   192         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 25, 25, 64)   192         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 25, 25, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 25, 25, 64)   192         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 25, 25, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 25, 25, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 25, 25, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 25, 25, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 25, 25, 288)  0           activation_12[0][0]              \n",
            "                                                                 activation_14[0][0]              \n",
            "                                                                 activation_17[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 25, 25, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 25, 25, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 25, 25, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 25, 25, 96)   55296       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 25, 25, 48)   144         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 25, 25, 96)   288         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 25, 25, 48)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 25, 25, 96)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 25, 25, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 25, 25, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 25, 25, 64)   76800       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 25, 25, 96)   82944       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 25, 25, 64)   18432       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 25, 25, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 25, 25, 64)   192         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 25, 25, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 25, 25, 64)   192         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 25, 25, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 25, 25, 64)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 25, 25, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 25, 25, 64)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 25, 25, 288)  0           activation_19[0][0]              \n",
            "                                                                 activation_21[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 25, 25, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 25, 25, 64)   192         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 25, 25, 64)   0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 25, 25, 96)   55296       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 25, 25, 96)   288         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 25, 25, 96)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
            "                                                                 activation_29[0][0]              \n",
            "                                                                 max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
            "                                                                 activation_33[0][0]              \n",
            "                                                                 activation_38[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
            "                                                                 activation_43[0][0]              \n",
            "                                                                 activation_48[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
            "                                                                 activation_53[0][0]              \n",
            "                                                                 activation_58[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
            "                                                                 activation_63[0][0]              \n",
            "                                                                 activation_68[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
            "                                                                 activation_75[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
            "                                                                 activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
            "                                                                 activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 51200)        0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 51200)        0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            102402      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 21,905,186\n",
            "Trainable params: 102,402\n",
            "Non-trainable params: 21,802,784\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDdG0hOYO5oc"
      },
      "source": [
        "### **Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Sv70psaHr4"
      },
      "source": [
        "class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, threshold,th):\n",
        "        super(MyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.th=th\n",
        "    def on_epoch_end(self, epoch, logs=None): \n",
        "        val_acc = logs[\"val_accuracy\"]\n",
        "        acc=logs[\"accuracy\"]\n",
        "        if acc>=self.th:\n",
        "          if val_acc >= self.threshold:\n",
        "            self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyYMAc1IyZOL",
        "outputId": "8535ee3c-4bf1-4d1c-abbe-c1eccfe7deec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "my_callback = MyThresholdCallback(threshold=0.94,th=0.95)\n",
        "history = model.fit(train_aug.flow(X_train, y_train, batch_size=32),\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    validation_steps=len(X_test) / 32,\n",
        "                    steps_per_epoch=len(X_train) / 32,\n",
        "                    epochs=500,\n",
        "                    callbacks=[my_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "99/98 [==============================] - 37s 378ms/step - loss: 1.2275 - accuracy: 0.8400 - val_loss: 0.6657 - val_accuracy: 0.9033\n",
            "Epoch 2/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.2158 - accuracy: 0.8827 - val_loss: 0.6993 - val_accuracy: 0.9071\n",
            "Epoch 3/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 0.8082 - accuracy: 0.8872 - val_loss: 0.8469 - val_accuracy: 0.9046\n",
            "Epoch 4/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 0.8455 - accuracy: 0.8974 - val_loss: 0.5874 - val_accuracy: 0.9160\n",
            "Epoch 5/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.1052 - accuracy: 0.8865 - val_loss: 0.9384 - val_accuracy: 0.9109\n",
            "Epoch 6/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.4471 - accuracy: 0.8840 - val_loss: 0.7990 - val_accuracy: 0.9211\n",
            "Epoch 7/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 0.9967 - accuracy: 0.9123 - val_loss: 0.6459 - val_accuracy: 0.9224\n",
            "Epoch 8/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.9084 - accuracy: 0.8817 - val_loss: 0.7697 - val_accuracy: 0.9211\n",
            "Epoch 9/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.1378 - accuracy: 0.9066 - val_loss: 0.9627 - val_accuracy: 0.9135\n",
            "Epoch 10/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.0293 - accuracy: 0.9120 - val_loss: 1.2157 - val_accuracy: 0.9046\n",
            "Epoch 11/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 2.1481 - accuracy: 0.8881 - val_loss: 1.5586 - val_accuracy: 0.8982\n",
            "Epoch 12/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.3928 - accuracy: 0.9050 - val_loss: 0.9671 - val_accuracy: 0.9275\n",
            "Epoch 13/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.0541 - accuracy: 0.9232 - val_loss: 0.7035 - val_accuracy: 0.9389\n",
            "Epoch 14/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.0673 - accuracy: 0.9203 - val_loss: 1.3315 - val_accuracy: 0.8906\n",
            "Epoch 15/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.3359 - accuracy: 0.9098 - val_loss: 0.7729 - val_accuracy: 0.9186\n",
            "Epoch 16/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8919 - accuracy: 0.9021 - val_loss: 1.5013 - val_accuracy: 0.9084\n",
            "Epoch 17/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6035 - accuracy: 0.8996 - val_loss: 1.5364 - val_accuracy: 0.8969\n",
            "Epoch 18/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4437 - accuracy: 0.9098 - val_loss: 0.7390 - val_accuracy: 0.9377\n",
            "Epoch 19/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.4096 - accuracy: 0.9114 - val_loss: 1.7749 - val_accuracy: 0.8919\n",
            "Epoch 20/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.2979 - accuracy: 0.9158 - val_loss: 0.5741 - val_accuracy: 0.9415\n",
            "Epoch 21/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.0963 - accuracy: 0.9178 - val_loss: 1.1228 - val_accuracy: 0.9122\n",
            "Epoch 22/500\n",
            "99/98 [==============================] - 35s 359ms/step - loss: 1.1292 - accuracy: 0.9181 - val_loss: 0.7358 - val_accuracy: 0.9313\n",
            "Epoch 23/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 2.0653 - accuracy: 0.8967 - val_loss: 1.4956 - val_accuracy: 0.9046\n",
            "Epoch 24/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.2987 - accuracy: 0.9152 - val_loss: 0.8211 - val_accuracy: 0.9338\n",
            "Epoch 25/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.1530 - accuracy: 0.9299 - val_loss: 0.7455 - val_accuracy: 0.9262\n",
            "Epoch 26/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.2029 - accuracy: 0.9184 - val_loss: 1.4484 - val_accuracy: 0.9097\n",
            "Epoch 27/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.2071 - accuracy: 0.9206 - val_loss: 0.9359 - val_accuracy: 0.9211\n",
            "Epoch 28/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.3341 - accuracy: 0.9165 - val_loss: 0.7501 - val_accuracy: 0.9364\n",
            "Epoch 29/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.7911 - accuracy: 0.9111 - val_loss: 0.8294 - val_accuracy: 0.9377\n",
            "Epoch 30/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.4855 - accuracy: 0.9174 - val_loss: 2.2155 - val_accuracy: 0.8944\n",
            "Epoch 31/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.6925 - accuracy: 0.9111 - val_loss: 0.6997 - val_accuracy: 0.9440\n",
            "Epoch 32/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.3429 - accuracy: 0.9232 - val_loss: 1.2070 - val_accuracy: 0.9300\n",
            "Epoch 33/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.5046 - accuracy: 0.9133 - val_loss: 0.5958 - val_accuracy: 0.9504\n",
            "Epoch 34/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.1943 - accuracy: 0.9225 - val_loss: 0.7423 - val_accuracy: 0.9427\n",
            "Epoch 35/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.4551 - accuracy: 0.9200 - val_loss: 0.6310 - val_accuracy: 0.9389\n",
            "Epoch 36/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.0927 - accuracy: 0.9311 - val_loss: 0.7367 - val_accuracy: 0.9427\n",
            "Epoch 37/500\n",
            "99/98 [==============================] - 36s 359ms/step - loss: 1.6376 - accuracy: 0.9088 - val_loss: 0.9556 - val_accuracy: 0.9326\n",
            "Epoch 38/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.2661 - accuracy: 0.9260 - val_loss: 1.0027 - val_accuracy: 0.9338\n",
            "Epoch 39/500\n",
            "99/98 [==============================] - 36s 361ms/step - loss: 1.1748 - accuracy: 0.9267 - val_loss: 0.7759 - val_accuracy: 0.9427\n",
            "Epoch 40/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.1758 - accuracy: 0.9311 - val_loss: 0.7411 - val_accuracy: 0.9427\n",
            "Epoch 41/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.3663 - accuracy: 0.9245 - val_loss: 1.5184 - val_accuracy: 0.9148\n",
            "Epoch 42/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4694 - accuracy: 0.9190 - val_loss: 2.1167 - val_accuracy: 0.8880\n",
            "Epoch 43/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.5855 - accuracy: 0.9213 - val_loss: 0.8271 - val_accuracy: 0.9338\n",
            "Epoch 44/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5138 - accuracy: 0.9193 - val_loss: 1.1756 - val_accuracy: 0.9211\n",
            "Epoch 45/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.4311 - accuracy: 0.9216 - val_loss: 1.2086 - val_accuracy: 0.9262\n",
            "Epoch 46/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.3743 - accuracy: 0.9248 - val_loss: 0.9681 - val_accuracy: 0.9288\n",
            "Epoch 47/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.3262 - accuracy: 0.9270 - val_loss: 1.1351 - val_accuracy: 0.9338\n",
            "Epoch 48/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.6099 - accuracy: 0.9187 - val_loss: 0.8680 - val_accuracy: 0.9351\n",
            "Epoch 49/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.2375 - accuracy: 0.9248 - val_loss: 1.5180 - val_accuracy: 0.9084\n",
            "Epoch 50/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4026 - accuracy: 0.9200 - val_loss: 0.9299 - val_accuracy: 0.9389\n",
            "Epoch 51/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7431 - accuracy: 0.9114 - val_loss: 0.8806 - val_accuracy: 0.9364\n",
            "Epoch 52/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.2679 - accuracy: 0.9273 - val_loss: 1.4305 - val_accuracy: 0.9224\n",
            "Epoch 53/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.3970 - accuracy: 0.9254 - val_loss: 0.7761 - val_accuracy: 0.9351\n",
            "Epoch 54/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.2336 - accuracy: 0.9343 - val_loss: 1.2050 - val_accuracy: 0.9275\n",
            "Epoch 55/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4058 - accuracy: 0.9286 - val_loss: 1.1987 - val_accuracy: 0.9326\n",
            "Epoch 56/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5345 - accuracy: 0.9260 - val_loss: 0.9436 - val_accuracy: 0.9377\n",
            "Epoch 57/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5149 - accuracy: 0.9193 - val_loss: 0.9919 - val_accuracy: 0.9415\n",
            "Epoch 58/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.4042 - accuracy: 0.9283 - val_loss: 0.9094 - val_accuracy: 0.9415\n",
            "Epoch 59/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5830 - accuracy: 0.9219 - val_loss: 1.5273 - val_accuracy: 0.9288\n",
            "Epoch 60/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.2842 - accuracy: 0.9232 - val_loss: 2.0073 - val_accuracy: 0.9135\n",
            "Epoch 61/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.5849 - accuracy: 0.9241 - val_loss: 1.1436 - val_accuracy: 0.9300\n",
            "Epoch 62/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.5450 - accuracy: 0.9264 - val_loss: 1.0096 - val_accuracy: 0.9326\n",
            "Epoch 63/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.4654 - accuracy: 0.9197 - val_loss: 1.1000 - val_accuracy: 0.9288\n",
            "Epoch 64/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.5123 - accuracy: 0.9254 - val_loss: 0.9594 - val_accuracy: 0.9427\n",
            "Epoch 65/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.2769 - accuracy: 0.9273 - val_loss: 1.0438 - val_accuracy: 0.9440\n",
            "Epoch 66/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5844 - accuracy: 0.9264 - val_loss: 1.0472 - val_accuracy: 0.9338\n",
            "Epoch 67/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.7692 - accuracy: 0.9197 - val_loss: 0.9825 - val_accuracy: 0.9415\n",
            "Epoch 68/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6385 - accuracy: 0.9203 - val_loss: 1.2310 - val_accuracy: 0.9326\n",
            "Epoch 69/500\n",
            "99/98 [==============================] - 35s 359ms/step - loss: 1.5459 - accuracy: 0.9276 - val_loss: 1.1282 - val_accuracy: 0.9389\n",
            "Epoch 70/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.2533 - accuracy: 0.9283 - val_loss: 1.3491 - val_accuracy: 0.9288\n",
            "Epoch 71/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4726 - accuracy: 0.9292 - val_loss: 1.2359 - val_accuracy: 0.9389\n",
            "Epoch 72/500\n",
            "99/98 [==============================] - 36s 360ms/step - loss: 1.4370 - accuracy: 0.9318 - val_loss: 1.6273 - val_accuracy: 0.9173\n",
            "Epoch 73/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.4553 - accuracy: 0.9280 - val_loss: 1.0500 - val_accuracy: 0.9389\n",
            "Epoch 74/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.6106 - accuracy: 0.9190 - val_loss: 1.1809 - val_accuracy: 0.9262\n",
            "Epoch 75/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6695 - accuracy: 0.9206 - val_loss: 1.6705 - val_accuracy: 0.9288\n",
            "Epoch 76/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.3703 - accuracy: 0.9302 - val_loss: 1.2607 - val_accuracy: 0.9415\n",
            "Epoch 77/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6666 - accuracy: 0.9235 - val_loss: 1.4011 - val_accuracy: 0.9402\n",
            "Epoch 78/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5806 - accuracy: 0.9283 - val_loss: 2.2810 - val_accuracy: 0.8944\n",
            "Epoch 79/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4420 - accuracy: 0.9241 - val_loss: 2.3161 - val_accuracy: 0.9097\n",
            "Epoch 80/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.9440 - accuracy: 0.9190 - val_loss: 1.4588 - val_accuracy: 0.9377\n",
            "Epoch 81/500\n",
            "99/98 [==============================] - 36s 362ms/step - loss: 1.5042 - accuracy: 0.9321 - val_loss: 1.1452 - val_accuracy: 0.9427\n",
            "Epoch 82/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.3954 - accuracy: 0.9302 - val_loss: 0.9775 - val_accuracy: 0.9415\n",
            "Epoch 83/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4989 - accuracy: 0.9334 - val_loss: 2.0134 - val_accuracy: 0.9020\n",
            "Epoch 84/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.9179 - accuracy: 0.9213 - val_loss: 1.8826 - val_accuracy: 0.9313\n",
            "Epoch 85/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8444 - accuracy: 0.9235 - val_loss: 1.1077 - val_accuracy: 0.9517\n",
            "Epoch 86/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6728 - accuracy: 0.9248 - val_loss: 0.9502 - val_accuracy: 0.9491\n",
            "Epoch 87/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6842 - accuracy: 0.9254 - val_loss: 1.0655 - val_accuracy: 0.9466\n",
            "Epoch 88/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.5899 - accuracy: 0.9238 - val_loss: 1.0289 - val_accuracy: 0.9415\n",
            "Epoch 89/500\n",
            "99/98 [==============================] - 36s 359ms/step - loss: 1.7904 - accuracy: 0.9257 - val_loss: 1.0862 - val_accuracy: 0.9415\n",
            "Epoch 90/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.6124 - accuracy: 0.9248 - val_loss: 1.1701 - val_accuracy: 0.9504\n",
            "Epoch 91/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4364 - accuracy: 0.9359 - val_loss: 1.1768 - val_accuracy: 0.9427\n",
            "Epoch 92/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.5127 - accuracy: 0.9311 - val_loss: 1.2398 - val_accuracy: 0.9440\n",
            "Epoch 93/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1970 - accuracy: 0.9165 - val_loss: 1.4980 - val_accuracy: 0.9338\n",
            "Epoch 94/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9782 - accuracy: 0.9245 - val_loss: 1.2221 - val_accuracy: 0.9453\n",
            "Epoch 95/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.3387 - accuracy: 0.9066 - val_loss: 1.3357 - val_accuracy: 0.9491\n",
            "Epoch 96/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.3630 - accuracy: 0.9359 - val_loss: 1.1511 - val_accuracy: 0.9415\n",
            "Epoch 97/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7070 - accuracy: 0.9257 - val_loss: 1.2394 - val_accuracy: 0.9453\n",
            "Epoch 98/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.7391 - accuracy: 0.9270 - val_loss: 2.9039 - val_accuracy: 0.9046\n",
            "Epoch 99/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.4229 - accuracy: 0.9359 - val_loss: 1.8534 - val_accuracy: 0.9237\n",
            "Epoch 100/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6811 - accuracy: 0.9302 - val_loss: 1.4599 - val_accuracy: 0.9300\n",
            "Epoch 101/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.6038 - accuracy: 0.9372 - val_loss: 1.0747 - val_accuracy: 0.9453\n",
            "Epoch 102/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.8919 - accuracy: 0.9251 - val_loss: 1.3316 - val_accuracy: 0.9377\n",
            "Epoch 103/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.9908 - accuracy: 0.9238 - val_loss: 1.1517 - val_accuracy: 0.9402\n",
            "Epoch 104/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9060 - accuracy: 0.9216 - val_loss: 1.7317 - val_accuracy: 0.9211\n",
            "Epoch 105/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.4085 - accuracy: 0.9356 - val_loss: 1.3814 - val_accuracy: 0.9389\n",
            "Epoch 106/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7506 - accuracy: 0.9305 - val_loss: 1.4204 - val_accuracy: 0.9288\n",
            "Epoch 107/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5453 - accuracy: 0.9359 - val_loss: 1.4888 - val_accuracy: 0.9237\n",
            "Epoch 108/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6562 - accuracy: 0.9296 - val_loss: 1.4872 - val_accuracy: 0.9415\n",
            "Epoch 109/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.7533 - accuracy: 0.9375 - val_loss: 1.7111 - val_accuracy: 0.9224\n",
            "Epoch 110/500\n",
            "99/98 [==============================] - 35s 350ms/step - loss: 1.6116 - accuracy: 0.9270 - val_loss: 1.3203 - val_accuracy: 0.9389\n",
            "Epoch 111/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.5727 - accuracy: 0.9356 - val_loss: 2.4587 - val_accuracy: 0.9071\n",
            "Epoch 112/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 2.1457 - accuracy: 0.9213 - val_loss: 1.7204 - val_accuracy: 0.9364\n",
            "Epoch 113/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 2.0187 - accuracy: 0.9267 - val_loss: 2.5397 - val_accuracy: 0.9186\n",
            "Epoch 114/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.6050 - accuracy: 0.9366 - val_loss: 1.2539 - val_accuracy: 0.9440\n",
            "Epoch 115/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9055 - accuracy: 0.9273 - val_loss: 1.7555 - val_accuracy: 0.9326\n",
            "Epoch 116/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 2.0709 - accuracy: 0.9264 - val_loss: 1.9928 - val_accuracy: 0.9186\n",
            "Epoch 117/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.6168 - accuracy: 0.9305 - val_loss: 1.1712 - val_accuracy: 0.9478\n",
            "Epoch 118/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4309 - accuracy: 0.9359 - val_loss: 1.1655 - val_accuracy: 0.9415\n",
            "Epoch 119/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.8536 - accuracy: 0.9343 - val_loss: 1.2638 - val_accuracy: 0.9402\n",
            "Epoch 120/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7555 - accuracy: 0.9321 - val_loss: 1.3836 - val_accuracy: 0.9300\n",
            "Epoch 121/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.6386 - accuracy: 0.9315 - val_loss: 1.0100 - val_accuracy: 0.9466\n",
            "Epoch 122/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 1.7227 - accuracy: 0.9308 - val_loss: 1.3664 - val_accuracy: 0.9491\n",
            "Epoch 123/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7762 - accuracy: 0.9286 - val_loss: 1.2621 - val_accuracy: 0.9377\n",
            "Epoch 124/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.5795 - accuracy: 0.9343 - val_loss: 1.5264 - val_accuracy: 0.9338\n",
            "Epoch 125/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7539 - accuracy: 0.9324 - val_loss: 2.8203 - val_accuracy: 0.9020\n",
            "Epoch 126/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6217 - accuracy: 0.9299 - val_loss: 1.1992 - val_accuracy: 0.9415\n",
            "Epoch 127/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5273 - accuracy: 0.9353 - val_loss: 0.9076 - val_accuracy: 0.9555\n",
            "Epoch 128/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5770 - accuracy: 0.9305 - val_loss: 1.1856 - val_accuracy: 0.9453\n",
            "Epoch 129/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6014 - accuracy: 0.9356 - val_loss: 1.2314 - val_accuracy: 0.9478\n",
            "Epoch 130/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.5387 - accuracy: 0.9299 - val_loss: 1.1783 - val_accuracy: 0.9478\n",
            "Epoch 131/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.8582 - accuracy: 0.9254 - val_loss: 1.1351 - val_accuracy: 0.9504\n",
            "Epoch 132/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6630 - accuracy: 0.9305 - val_loss: 2.0579 - val_accuracy: 0.9160\n",
            "Epoch 133/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.0774 - accuracy: 0.9286 - val_loss: 1.2223 - val_accuracy: 0.9504\n",
            "Epoch 134/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 3.0595 - accuracy: 0.9114 - val_loss: 1.8805 - val_accuracy: 0.9351\n",
            "Epoch 135/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.7087 - accuracy: 0.9334 - val_loss: 1.4414 - val_accuracy: 0.9389\n",
            "Epoch 136/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7526 - accuracy: 0.9353 - val_loss: 1.3637 - val_accuracy: 0.9453\n",
            "Epoch 137/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.4250 - accuracy: 0.9382 - val_loss: 2.1236 - val_accuracy: 0.9237\n",
            "Epoch 138/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.8910 - accuracy: 0.9334 - val_loss: 1.3779 - val_accuracy: 0.9377\n",
            "Epoch 139/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.8666 - accuracy: 0.9286 - val_loss: 1.2010 - val_accuracy: 0.9478\n",
            "Epoch 140/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7085 - accuracy: 0.9331 - val_loss: 1.1263 - val_accuracy: 0.9491\n",
            "Epoch 141/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.6913 - accuracy: 0.9343 - val_loss: 1.7357 - val_accuracy: 0.9351\n",
            "Epoch 142/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.5848 - accuracy: 0.9372 - val_loss: 1.3961 - val_accuracy: 0.9338\n",
            "Epoch 143/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 1.9595 - accuracy: 0.9302 - val_loss: 1.2002 - val_accuracy: 0.9427\n",
            "Epoch 144/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 1.6558 - accuracy: 0.9343 - val_loss: 1.2506 - val_accuracy: 0.9402\n",
            "Epoch 145/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 1.7702 - accuracy: 0.9334 - val_loss: 1.5183 - val_accuracy: 0.9275\n",
            "Epoch 146/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.6941 - accuracy: 0.9359 - val_loss: 1.6991 - val_accuracy: 0.9288\n",
            "Epoch 147/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.9516 - accuracy: 0.9334 - val_loss: 1.0849 - val_accuracy: 0.9453\n",
            "Epoch 148/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8579 - accuracy: 0.9264 - val_loss: 1.3110 - val_accuracy: 0.9338\n",
            "Epoch 149/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9812 - accuracy: 0.9276 - val_loss: 1.2892 - val_accuracy: 0.9415\n",
            "Epoch 150/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.5656 - accuracy: 0.9366 - val_loss: 1.1553 - val_accuracy: 0.9466\n",
            "Epoch 151/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7916 - accuracy: 0.9353 - val_loss: 1.2962 - val_accuracy: 0.9402\n",
            "Epoch 152/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6141 - accuracy: 0.9369 - val_loss: 1.3342 - val_accuracy: 0.9478\n",
            "Epoch 153/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9284 - accuracy: 0.9337 - val_loss: 2.2771 - val_accuracy: 0.9211\n",
            "Epoch 154/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.9203 - accuracy: 0.9273 - val_loss: 1.2462 - val_accuracy: 0.9453\n",
            "Epoch 155/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9386 - accuracy: 0.9343 - val_loss: 2.2683 - val_accuracy: 0.9198\n",
            "Epoch 156/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8596 - accuracy: 0.9359 - val_loss: 1.1121 - val_accuracy: 0.9453\n",
            "Epoch 157/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.6904 - accuracy: 0.9311 - val_loss: 1.6045 - val_accuracy: 0.9300\n",
            "Epoch 158/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8207 - accuracy: 0.9327 - val_loss: 1.5318 - val_accuracy: 0.9377\n",
            "Epoch 159/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.5551 - accuracy: 0.9394 - val_loss: 1.1645 - val_accuracy: 0.9427\n",
            "Epoch 160/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6258 - accuracy: 0.9334 - val_loss: 1.1958 - val_accuracy: 0.9415\n",
            "Epoch 161/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9464 - accuracy: 0.9276 - val_loss: 1.0832 - val_accuracy: 0.9453\n",
            "Epoch 162/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7551 - accuracy: 0.9305 - val_loss: 1.8901 - val_accuracy: 0.9300\n",
            "Epoch 163/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.5656 - accuracy: 0.9413 - val_loss: 1.0284 - val_accuracy: 0.9478\n",
            "Epoch 164/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.9064 - accuracy: 0.9308 - val_loss: 1.3761 - val_accuracy: 0.9351\n",
            "Epoch 165/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8509 - accuracy: 0.9289 - val_loss: 1.9909 - val_accuracy: 0.9237\n",
            "Epoch 166/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1160 - accuracy: 0.9280 - val_loss: 2.0488 - val_accuracy: 0.9135\n",
            "Epoch 167/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7352 - accuracy: 0.9343 - val_loss: 1.0752 - val_accuracy: 0.9453\n",
            "Epoch 168/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6051 - accuracy: 0.9366 - val_loss: 0.9926 - val_accuracy: 0.9491\n",
            "Epoch 169/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4899 - accuracy: 0.9369 - val_loss: 1.2019 - val_accuracy: 0.9402\n",
            "Epoch 170/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.1384 - accuracy: 0.9305 - val_loss: 1.2016 - val_accuracy: 0.9402\n",
            "Epoch 171/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.4389 - accuracy: 0.9433 - val_loss: 1.8973 - val_accuracy: 0.9198\n",
            "Epoch 172/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8483 - accuracy: 0.9356 - val_loss: 1.2437 - val_accuracy: 0.9440\n",
            "Epoch 173/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7209 - accuracy: 0.9315 - val_loss: 3.5586 - val_accuracy: 0.8969\n",
            "Epoch 174/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8245 - accuracy: 0.9296 - val_loss: 1.7849 - val_accuracy: 0.9389\n",
            "Epoch 175/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 2.2133 - accuracy: 0.9245 - val_loss: 1.9107 - val_accuracy: 0.9427\n",
            "Epoch 176/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.4731 - accuracy: 0.9420 - val_loss: 1.4958 - val_accuracy: 0.9440\n",
            "Epoch 177/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8488 - accuracy: 0.9337 - val_loss: 1.8298 - val_accuracy: 0.9249\n",
            "Epoch 178/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.8102 - accuracy: 0.9378 - val_loss: 2.8910 - val_accuracy: 0.8919\n",
            "Epoch 179/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7222 - accuracy: 0.9378 - val_loss: 1.4687 - val_accuracy: 0.9402\n",
            "Epoch 180/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9034 - accuracy: 0.9343 - val_loss: 1.4730 - val_accuracy: 0.9415\n",
            "Epoch 181/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7702 - accuracy: 0.9356 - val_loss: 2.4986 - val_accuracy: 0.9186\n",
            "Epoch 182/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5913 - accuracy: 0.9385 - val_loss: 1.3501 - val_accuracy: 0.9415\n",
            "Epoch 183/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8304 - accuracy: 0.9308 - val_loss: 3.2196 - val_accuracy: 0.9071\n",
            "Epoch 184/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8519 - accuracy: 0.9321 - val_loss: 1.2126 - val_accuracy: 0.9440\n",
            "Epoch 185/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7935 - accuracy: 0.9337 - val_loss: 1.4114 - val_accuracy: 0.9338\n",
            "Epoch 186/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8868 - accuracy: 0.9343 - val_loss: 2.4024 - val_accuracy: 0.9288\n",
            "Epoch 187/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.1595 - accuracy: 0.9260 - val_loss: 1.4303 - val_accuracy: 0.9364\n",
            "Epoch 188/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7898 - accuracy: 0.9350 - val_loss: 1.3942 - val_accuracy: 0.9415\n",
            "Epoch 189/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.6393 - accuracy: 0.9382 - val_loss: 1.6797 - val_accuracy: 0.9402\n",
            "Epoch 190/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8390 - accuracy: 0.9286 - val_loss: 1.2880 - val_accuracy: 0.9338\n",
            "Epoch 191/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6792 - accuracy: 0.9369 - val_loss: 1.0998 - val_accuracy: 0.9491\n",
            "Epoch 192/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6431 - accuracy: 0.9340 - val_loss: 1.8440 - val_accuracy: 0.9300\n",
            "Epoch 193/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 2.4247 - accuracy: 0.9225 - val_loss: 1.3381 - val_accuracy: 0.9364\n",
            "Epoch 194/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7658 - accuracy: 0.9337 - val_loss: 1.3791 - val_accuracy: 0.9440\n",
            "Epoch 195/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.6690 - accuracy: 0.9385 - val_loss: 1.1756 - val_accuracy: 0.9427\n",
            "Epoch 196/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7600 - accuracy: 0.9356 - val_loss: 1.2849 - val_accuracy: 0.9478\n",
            "Epoch 197/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 2.0313 - accuracy: 0.9308 - val_loss: 1.1056 - val_accuracy: 0.9466\n",
            "Epoch 198/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 2.1738 - accuracy: 0.9318 - val_loss: 1.1404 - val_accuracy: 0.9491\n",
            "Epoch 199/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 2.0742 - accuracy: 0.9299 - val_loss: 1.8655 - val_accuracy: 0.9402\n",
            "Epoch 200/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7600 - accuracy: 0.9436 - val_loss: 1.5340 - val_accuracy: 0.9389\n",
            "Epoch 201/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9976 - accuracy: 0.9334 - val_loss: 1.3278 - val_accuracy: 0.9504\n",
            "Epoch 202/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.9727 - accuracy: 0.9340 - val_loss: 1.6383 - val_accuracy: 0.9402\n",
            "Epoch 203/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.5752 - accuracy: 0.9273 - val_loss: 3.3903 - val_accuracy: 0.8880\n",
            "Epoch 204/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.8658 - accuracy: 0.9155 - val_loss: 1.4318 - val_accuracy: 0.9440\n",
            "Epoch 205/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1667 - accuracy: 0.9327 - val_loss: 1.0729 - val_accuracy: 0.9529\n",
            "Epoch 206/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8225 - accuracy: 0.9311 - val_loss: 1.4478 - val_accuracy: 0.9478\n",
            "Epoch 207/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9577 - accuracy: 0.9378 - val_loss: 1.1969 - val_accuracy: 0.9491\n",
            "Epoch 208/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7165 - accuracy: 0.9347 - val_loss: 1.0169 - val_accuracy: 0.9478\n",
            "Epoch 209/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7324 - accuracy: 0.9385 - val_loss: 1.1438 - val_accuracy: 0.9440\n",
            "Epoch 210/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 2.7492 - accuracy: 0.9213 - val_loss: 2.0140 - val_accuracy: 0.9389\n",
            "Epoch 211/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8350 - accuracy: 0.9382 - val_loss: 1.6429 - val_accuracy: 0.9529\n",
            "Epoch 212/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8341 - accuracy: 0.9353 - val_loss: 1.4778 - val_accuracy: 0.9504\n",
            "Epoch 213/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 1.6020 - accuracy: 0.9404 - val_loss: 1.2125 - val_accuracy: 0.9478\n",
            "Epoch 214/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.5885 - accuracy: 0.9461 - val_loss: 1.2201 - val_accuracy: 0.9542\n",
            "Epoch 215/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.4495 - accuracy: 0.9474 - val_loss: 1.2211 - val_accuracy: 0.9364\n",
            "Epoch 216/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7514 - accuracy: 0.9378 - val_loss: 1.5345 - val_accuracy: 0.9491\n",
            "Epoch 217/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.0621 - accuracy: 0.9311 - val_loss: 1.3282 - val_accuracy: 0.9593\n",
            "Epoch 218/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.8233 - accuracy: 0.9305 - val_loss: 1.7756 - val_accuracy: 0.9440\n",
            "Epoch 219/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9728 - accuracy: 0.9359 - val_loss: 1.5012 - val_accuracy: 0.9466\n",
            "Epoch 220/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9429 - accuracy: 0.9315 - val_loss: 1.3578 - val_accuracy: 0.9466\n",
            "Epoch 221/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8687 - accuracy: 0.9385 - val_loss: 1.1006 - val_accuracy: 0.9478\n",
            "Epoch 222/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7314 - accuracy: 0.9350 - val_loss: 2.3407 - val_accuracy: 0.9275\n",
            "Epoch 223/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7042 - accuracy: 0.9413 - val_loss: 2.6486 - val_accuracy: 0.9211\n",
            "Epoch 224/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7852 - accuracy: 0.9375 - val_loss: 1.4740 - val_accuracy: 0.9453\n",
            "Epoch 225/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7777 - accuracy: 0.9353 - val_loss: 1.2187 - val_accuracy: 0.9555\n",
            "Epoch 226/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8375 - accuracy: 0.9299 - val_loss: 1.3683 - val_accuracy: 0.9466\n",
            "Epoch 227/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9081 - accuracy: 0.9315 - val_loss: 1.3908 - val_accuracy: 0.9453\n",
            "Epoch 228/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9191 - accuracy: 0.9372 - val_loss: 2.2230 - val_accuracy: 0.9389\n",
            "Epoch 229/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.5662 - accuracy: 0.9350 - val_loss: 1.5643 - val_accuracy: 0.9427\n",
            "Epoch 230/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1317 - accuracy: 0.9343 - val_loss: 1.4295 - val_accuracy: 0.9453\n",
            "Epoch 231/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.9454 - accuracy: 0.9324 - val_loss: 1.3844 - val_accuracy: 0.9466\n",
            "Epoch 232/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.6365 - accuracy: 0.9426 - val_loss: 2.7004 - val_accuracy: 0.9224\n",
            "Epoch 233/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.7980 - accuracy: 0.9394 - val_loss: 1.8180 - val_accuracy: 0.9288\n",
            "Epoch 234/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8130 - accuracy: 0.9382 - val_loss: 1.4035 - val_accuracy: 0.9440\n",
            "Epoch 235/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 1.8924 - accuracy: 0.9337 - val_loss: 1.2008 - val_accuracy: 0.9529\n",
            "Epoch 236/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8192 - accuracy: 0.9366 - val_loss: 1.4382 - val_accuracy: 0.9427\n",
            "Epoch 237/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8420 - accuracy: 0.9369 - val_loss: 1.5987 - val_accuracy: 0.9402\n",
            "Epoch 238/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8989 - accuracy: 0.9391 - val_loss: 2.3917 - val_accuracy: 0.9275\n",
            "Epoch 239/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.6045 - accuracy: 0.9407 - val_loss: 1.3160 - val_accuracy: 0.9529\n",
            "Epoch 240/500\n",
            "99/98 [==============================] - 35s 353ms/step - loss: 2.0364 - accuracy: 0.9324 - val_loss: 1.9078 - val_accuracy: 0.9351\n",
            "Epoch 241/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.9312 - accuracy: 0.9299 - val_loss: 1.0353 - val_accuracy: 0.9631\n",
            "Epoch 242/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.7330 - accuracy: 0.9359 - val_loss: 1.4005 - val_accuracy: 0.9427\n",
            "Epoch 243/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.8614 - accuracy: 0.9340 - val_loss: 3.1869 - val_accuracy: 0.9148\n",
            "Epoch 244/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7420 - accuracy: 0.9382 - val_loss: 1.3559 - val_accuracy: 0.9517\n",
            "Epoch 245/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.6044 - accuracy: 0.9423 - val_loss: 2.3668 - val_accuracy: 0.9262\n",
            "Epoch 246/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.6513 - accuracy: 0.9423 - val_loss: 1.1611 - val_accuracy: 0.9517\n",
            "Epoch 247/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9243 - accuracy: 0.9372 - val_loss: 1.6283 - val_accuracy: 0.9351\n",
            "Epoch 248/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.8399 - accuracy: 0.9270 - val_loss: 2.6708 - val_accuracy: 0.8995\n",
            "Epoch 249/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.0021 - accuracy: 0.9369 - val_loss: 1.3153 - val_accuracy: 0.9504\n",
            "Epoch 250/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.2636 - accuracy: 0.9273 - val_loss: 1.3310 - val_accuracy: 0.9466\n",
            "Epoch 251/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 2.0640 - accuracy: 0.9315 - val_loss: 1.4188 - val_accuracy: 0.9478\n",
            "Epoch 252/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8417 - accuracy: 0.9388 - val_loss: 1.2999 - val_accuracy: 0.9491\n",
            "Epoch 253/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.1114 - accuracy: 0.9311 - val_loss: 1.1876 - val_accuracy: 0.9517\n",
            "Epoch 254/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8045 - accuracy: 0.9359 - val_loss: 1.1848 - val_accuracy: 0.9504\n",
            "Epoch 255/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1947 - accuracy: 0.9299 - val_loss: 3.1583 - val_accuracy: 0.8969\n",
            "Epoch 256/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.9677 - accuracy: 0.9174 - val_loss: 1.2545 - val_accuracy: 0.9542\n",
            "Epoch 257/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.3544 - accuracy: 0.9305 - val_loss: 1.1420 - val_accuracy: 0.9555\n",
            "Epoch 258/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.7512 - accuracy: 0.9382 - val_loss: 1.2046 - val_accuracy: 0.9491\n",
            "Epoch 259/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9597 - accuracy: 0.9362 - val_loss: 1.2133 - val_accuracy: 0.9542\n",
            "Epoch 260/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.2490 - accuracy: 0.9334 - val_loss: 1.3242 - val_accuracy: 0.9529\n",
            "Epoch 261/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.7322 - accuracy: 0.9417 - val_loss: 1.2690 - val_accuracy: 0.9517\n",
            "Epoch 262/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.7862 - accuracy: 0.9356 - val_loss: 2.1353 - val_accuracy: 0.9288\n",
            "Epoch 263/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.6337 - accuracy: 0.9429 - val_loss: 1.4345 - val_accuracy: 0.9491\n",
            "Epoch 264/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.9137 - accuracy: 0.9404 - val_loss: 1.1629 - val_accuracy: 0.9517\n",
            "Epoch 265/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 2.1800 - accuracy: 0.9296 - val_loss: 1.6532 - val_accuracy: 0.9389\n",
            "Epoch 266/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 1.8831 - accuracy: 0.9398 - val_loss: 1.6634 - val_accuracy: 0.9377\n",
            "Epoch 267/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 1.9998 - accuracy: 0.9372 - val_loss: 2.8268 - val_accuracy: 0.9148\n",
            "Epoch 268/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1146 - accuracy: 0.9321 - val_loss: 1.4197 - val_accuracy: 0.9453\n",
            "Epoch 269/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.9378 - accuracy: 0.9394 - val_loss: 1.6025 - val_accuracy: 0.9351\n",
            "Epoch 270/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 2.1110 - accuracy: 0.9302 - val_loss: 1.6668 - val_accuracy: 0.9351\n",
            "Epoch 271/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.7715 - accuracy: 0.9455 - val_loss: 2.3979 - val_accuracy: 0.9186\n",
            "Epoch 272/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8003 - accuracy: 0.9417 - val_loss: 1.1855 - val_accuracy: 0.9466\n",
            "Epoch 273/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.6424 - accuracy: 0.9213 - val_loss: 1.5786 - val_accuracy: 0.9351\n",
            "Epoch 274/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 1.8910 - accuracy: 0.9417 - val_loss: 1.2184 - val_accuracy: 0.9542\n",
            "Epoch 275/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.0825 - accuracy: 0.9366 - val_loss: 1.0564 - val_accuracy: 0.9555\n",
            "Epoch 276/500\n",
            "99/98 [==============================] - 35s 356ms/step - loss: 2.4755 - accuracy: 0.9315 - val_loss: 1.9202 - val_accuracy: 0.9326\n",
            "Epoch 277/500\n",
            "99/98 [==============================] - 35s 355ms/step - loss: 2.1833 - accuracy: 0.9362 - val_loss: 1.1664 - val_accuracy: 0.9567\n",
            "Epoch 278/500\n",
            "99/98 [==============================] - 35s 357ms/step - loss: 1.8475 - accuracy: 0.9356 - val_loss: 1.4960 - val_accuracy: 0.9402\n",
            "Epoch 279/500\n",
            "99/98 [==============================] - 35s 354ms/step - loss: 2.1491 - accuracy: 0.9337 - val_loss: 3.0458 - val_accuracy: 0.9135\n",
            "Epoch 280/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.0278 - accuracy: 0.9404 - val_loss: 1.3247 - val_accuracy: 0.9542\n",
            "Epoch 281/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.8960 - accuracy: 0.9372 - val_loss: 1.1181 - val_accuracy: 0.9580\n",
            "Epoch 282/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.9718 - accuracy: 0.9378 - val_loss: 1.5727 - val_accuracy: 0.9504\n",
            "Epoch 283/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 1.8633 - accuracy: 0.9366 - val_loss: 1.3453 - val_accuracy: 0.9567\n",
            "Epoch 284/500\n",
            "99/98 [==============================] - 35s 358ms/step - loss: 2.7478 - accuracy: 0.9292 - val_loss: 1.9821 - val_accuracy: 0.9122\n",
            "Epoch 285/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 2.3310 - accuracy: 0.9280 - val_loss: 1.3319 - val_accuracy: 0.9567\n",
            "Epoch 286/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.7784 - accuracy: 0.9420 - val_loss: 1.3281 - val_accuracy: 0.9427\n",
            "Epoch 287/500\n",
            "99/98 [==============================] - 34s 338ms/step - loss: 1.9123 - accuracy: 0.9353 - val_loss: 1.3550 - val_accuracy: 0.9517\n",
            "Epoch 288/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.2816 - accuracy: 0.9289 - val_loss: 1.1952 - val_accuracy: 0.9529\n",
            "Epoch 289/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.1483 - accuracy: 0.9308 - val_loss: 1.1816 - val_accuracy: 0.9491\n",
            "Epoch 290/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.8985 - accuracy: 0.9350 - val_loss: 1.7502 - val_accuracy: 0.9389\n",
            "Epoch 291/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 2.2472 - accuracy: 0.9299 - val_loss: 2.5910 - val_accuracy: 0.9237\n",
            "Epoch 292/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.1957 - accuracy: 0.9264 - val_loss: 1.3384 - val_accuracy: 0.9478\n",
            "Epoch 293/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.9891 - accuracy: 0.9369 - val_loss: 1.3020 - val_accuracy: 0.9567\n",
            "Epoch 294/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.3413 - accuracy: 0.9343 - val_loss: 2.4257 - val_accuracy: 0.9262\n",
            "Epoch 295/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.6498 - accuracy: 0.9413 - val_loss: 1.2668 - val_accuracy: 0.9529\n",
            "Epoch 296/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.8283 - accuracy: 0.9382 - val_loss: 1.4397 - val_accuracy: 0.9504\n",
            "Epoch 297/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.8369 - accuracy: 0.9442 - val_loss: 1.2662 - val_accuracy: 0.9491\n",
            "Epoch 298/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.7302 - accuracy: 0.9388 - val_loss: 1.4921 - val_accuracy: 0.9389\n",
            "Epoch 299/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.9946 - accuracy: 0.9398 - val_loss: 1.2676 - val_accuracy: 0.9517\n",
            "Epoch 300/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.1313 - accuracy: 0.9350 - val_loss: 3.0173 - val_accuracy: 0.9160\n",
            "Epoch 301/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.2754 - accuracy: 0.9318 - val_loss: 1.9547 - val_accuracy: 0.9326\n",
            "Epoch 302/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.7849 - accuracy: 0.9445 - val_loss: 1.5497 - val_accuracy: 0.9453\n",
            "Epoch 303/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.0754 - accuracy: 0.9366 - val_loss: 1.2099 - val_accuracy: 0.9618\n",
            "Epoch 304/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.9197 - accuracy: 0.9340 - val_loss: 1.5429 - val_accuracy: 0.9517\n",
            "Epoch 305/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.1459 - accuracy: 0.9366 - val_loss: 2.7817 - val_accuracy: 0.9224\n",
            "Epoch 306/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.6906 - accuracy: 0.9458 - val_loss: 1.2122 - val_accuracy: 0.9580\n",
            "Epoch 307/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 1.9499 - accuracy: 0.9410 - val_loss: 1.2135 - val_accuracy: 0.9555\n",
            "Epoch 308/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 1.7626 - accuracy: 0.9410 - val_loss: 1.3689 - val_accuracy: 0.9542\n",
            "Epoch 309/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.7892 - accuracy: 0.9404 - val_loss: 1.2272 - val_accuracy: 0.9555\n",
            "Epoch 310/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.9396 - accuracy: 0.9404 - val_loss: 1.3169 - val_accuracy: 0.9555\n",
            "Epoch 311/500\n",
            "99/98 [==============================] - 34s 347ms/step - loss: 2.4525 - accuracy: 0.9270 - val_loss: 2.3931 - val_accuracy: 0.9313\n",
            "Epoch 312/500\n",
            "99/98 [==============================] - 34s 344ms/step - loss: 1.8719 - accuracy: 0.9369 - val_loss: 1.3297 - val_accuracy: 0.9491\n",
            "Epoch 313/500\n",
            "99/98 [==============================] - 34s 346ms/step - loss: 2.1790 - accuracy: 0.9327 - val_loss: 1.4295 - val_accuracy: 0.9466\n",
            "Epoch 314/500\n",
            "99/98 [==============================] - 34s 345ms/step - loss: 2.2237 - accuracy: 0.9331 - val_loss: 1.4907 - val_accuracy: 0.9517\n",
            "Epoch 315/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.3712 - accuracy: 0.9311 - val_loss: 1.3384 - val_accuracy: 0.9504\n",
            "Epoch 316/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.1988 - accuracy: 0.9347 - val_loss: 1.9295 - val_accuracy: 0.9275\n",
            "Epoch 317/500\n",
            "99/98 [==============================] - 34s 344ms/step - loss: 1.9170 - accuracy: 0.9429 - val_loss: 2.5124 - val_accuracy: 0.9351\n",
            "Epoch 318/500\n",
            "99/98 [==============================] - 34s 345ms/step - loss: 1.9159 - accuracy: 0.9388 - val_loss: 1.5668 - val_accuracy: 0.9542\n",
            "Epoch 319/500\n",
            "99/98 [==============================] - 34s 344ms/step - loss: 1.5672 - accuracy: 0.9449 - val_loss: 2.9297 - val_accuracy: 0.9300\n",
            "Epoch 320/500\n",
            "99/98 [==============================] - 34s 344ms/step - loss: 2.0740 - accuracy: 0.9327 - val_loss: 1.8328 - val_accuracy: 0.9504\n",
            "Epoch 321/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.7134 - accuracy: 0.9468 - val_loss: 1.4864 - val_accuracy: 0.9517\n",
            "Epoch 322/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.7062 - accuracy: 0.9484 - val_loss: 2.2547 - val_accuracy: 0.9326\n",
            "Epoch 323/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 1.9227 - accuracy: 0.9347 - val_loss: 1.4306 - val_accuracy: 0.9491\n",
            "Epoch 324/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 1.8533 - accuracy: 0.9417 - val_loss: 1.8802 - val_accuracy: 0.9415\n",
            "Epoch 325/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.9884 - accuracy: 0.9327 - val_loss: 1.6987 - val_accuracy: 0.9377\n",
            "Epoch 326/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 1.9743 - accuracy: 0.9378 - val_loss: 1.4025 - val_accuracy: 0.9466\n",
            "Epoch 327/500\n",
            "99/98 [==============================] - 34s 343ms/step - loss: 1.8155 - accuracy: 0.9401 - val_loss: 1.4905 - val_accuracy: 0.9491\n",
            "Epoch 328/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.8762 - accuracy: 0.9413 - val_loss: 1.4409 - val_accuracy: 0.9491\n",
            "Epoch 329/500\n",
            "99/98 [==============================] - 34s 348ms/step - loss: 1.8867 - accuracy: 0.9404 - val_loss: 1.4417 - val_accuracy: 0.9440\n",
            "Epoch 330/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.8798 - accuracy: 0.9391 - val_loss: 1.3941 - val_accuracy: 0.9517\n",
            "Epoch 331/500\n",
            "99/98 [==============================] - 34s 348ms/step - loss: 1.8539 - accuracy: 0.9375 - val_loss: 1.3992 - val_accuracy: 0.9491\n",
            "Epoch 332/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.8261 - accuracy: 0.9375 - val_loss: 1.5536 - val_accuracy: 0.9555\n",
            "Epoch 333/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.6320 - accuracy: 0.9461 - val_loss: 1.3452 - val_accuracy: 0.9517\n",
            "Epoch 334/500\n",
            "99/98 [==============================] - 35s 350ms/step - loss: 1.9481 - accuracy: 0.9391 - val_loss: 1.4740 - val_accuracy: 0.9478\n",
            "Epoch 335/500\n",
            "99/98 [==============================] - 35s 350ms/step - loss: 1.6435 - accuracy: 0.9445 - val_loss: 3.3204 - val_accuracy: 0.9109\n",
            "Epoch 336/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 2.1550 - accuracy: 0.9315 - val_loss: 1.2814 - val_accuracy: 0.9453\n",
            "Epoch 337/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.8603 - accuracy: 0.9423 - val_loss: 1.4878 - val_accuracy: 0.9338\n",
            "Epoch 338/500\n",
            "99/98 [==============================] - 34s 348ms/step - loss: 1.3882 - accuracy: 0.9464 - val_loss: 1.4338 - val_accuracy: 0.9427\n",
            "Epoch 339/500\n",
            "99/98 [==============================] - 35s 352ms/step - loss: 2.3144 - accuracy: 0.9305 - val_loss: 1.9089 - val_accuracy: 0.9249\n",
            "Epoch 340/500\n",
            "99/98 [==============================] - 35s 350ms/step - loss: 2.0017 - accuracy: 0.9420 - val_loss: 2.3843 - val_accuracy: 0.9313\n",
            "Epoch 341/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.8752 - accuracy: 0.9385 - val_loss: 1.5730 - val_accuracy: 0.9377\n",
            "Epoch 342/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 1.7993 - accuracy: 0.9362 - val_loss: 2.0416 - val_accuracy: 0.9377\n",
            "Epoch 343/500\n",
            "99/98 [==============================] - 34s 348ms/step - loss: 2.0206 - accuracy: 0.9366 - val_loss: 1.9996 - val_accuracy: 0.9389\n",
            "Epoch 344/500\n",
            "99/98 [==============================] - 35s 349ms/step - loss: 2.1506 - accuracy: 0.9353 - val_loss: 4.4970 - val_accuracy: 0.8995\n",
            "Epoch 345/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 2.5301 - accuracy: 0.9283 - val_loss: 1.8990 - val_accuracy: 0.9300\n",
            "Epoch 346/500\n",
            "99/98 [==============================] - 35s 350ms/step - loss: 2.1287 - accuracy: 0.9331 - val_loss: 2.5247 - val_accuracy: 0.9224\n",
            "Epoch 347/500\n",
            "99/98 [==============================] - 35s 351ms/step - loss: 2.1231 - accuracy: 0.9391 - val_loss: 1.5909 - val_accuracy: 0.9466\n",
            "Epoch 348/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1397 - accuracy: 0.9362 - val_loss: 2.0195 - val_accuracy: 0.9364\n",
            "Epoch 349/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.1755 - accuracy: 0.9372 - val_loss: 1.5669 - val_accuracy: 0.9402\n",
            "Epoch 350/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8459 - accuracy: 0.9362 - val_loss: 1.9101 - val_accuracy: 0.9415\n",
            "Epoch 351/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.8210 - accuracy: 0.9445 - val_loss: 2.5702 - val_accuracy: 0.9313\n",
            "Epoch 352/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0826 - accuracy: 0.9369 - val_loss: 1.4695 - val_accuracy: 0.9491\n",
            "Epoch 353/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.8548 - accuracy: 0.9391 - val_loss: 2.3755 - val_accuracy: 0.9351\n",
            "Epoch 354/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0341 - accuracy: 0.9369 - val_loss: 1.6140 - val_accuracy: 0.9453\n",
            "Epoch 355/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.9741 - accuracy: 0.9429 - val_loss: 6.7774 - val_accuracy: 0.8842\n",
            "Epoch 356/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0799 - accuracy: 0.9337 - val_loss: 1.7696 - val_accuracy: 0.9440\n",
            "Epoch 357/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.6976 - accuracy: 0.9433 - val_loss: 2.0054 - val_accuracy: 0.9427\n",
            "Epoch 358/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1509 - accuracy: 0.9394 - val_loss: 1.8568 - val_accuracy: 0.9466\n",
            "Epoch 359/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.1572 - accuracy: 0.9331 - val_loss: 1.7513 - val_accuracy: 0.9466\n",
            "Epoch 360/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.9409 - accuracy: 0.9378 - val_loss: 2.0798 - val_accuracy: 0.9427\n",
            "Epoch 361/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.0383 - accuracy: 0.9401 - val_loss: 1.6464 - val_accuracy: 0.9427\n",
            "Epoch 362/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.0555 - accuracy: 0.9417 - val_loss: 1.7614 - val_accuracy: 0.9478\n",
            "Epoch 363/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.9402 - accuracy: 0.9378 - val_loss: 2.4234 - val_accuracy: 0.9338\n",
            "Epoch 364/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.2395 - accuracy: 0.9327 - val_loss: 1.5506 - val_accuracy: 0.9466\n",
            "Epoch 365/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.9288 - accuracy: 0.9417 - val_loss: 1.7409 - val_accuracy: 0.9440\n",
            "Epoch 366/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.0221 - accuracy: 0.9401 - val_loss: 1.8410 - val_accuracy: 0.9440\n",
            "Epoch 367/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.7792 - accuracy: 0.9442 - val_loss: 1.6422 - val_accuracy: 0.9478\n",
            "Epoch 368/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.8103 - accuracy: 0.9458 - val_loss: 1.5279 - val_accuracy: 0.9478\n",
            "Epoch 369/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.8903 - accuracy: 0.9420 - val_loss: 2.5467 - val_accuracy: 0.9351\n",
            "Epoch 370/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.5059 - accuracy: 0.9311 - val_loss: 4.3970 - val_accuracy: 0.8842\n",
            "Epoch 371/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.3178 - accuracy: 0.9340 - val_loss: 2.6406 - val_accuracy: 0.9313\n",
            "Epoch 372/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.8131 - accuracy: 0.9433 - val_loss: 4.4007 - val_accuracy: 0.9020\n",
            "Epoch 373/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.5627 - accuracy: 0.9292 - val_loss: 2.6030 - val_accuracy: 0.9249\n",
            "Epoch 374/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.0819 - accuracy: 0.9407 - val_loss: 1.9922 - val_accuracy: 0.9351\n",
            "Epoch 375/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.1347 - accuracy: 0.9420 - val_loss: 1.7467 - val_accuracy: 0.9427\n",
            "Epoch 376/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.0857 - accuracy: 0.9391 - val_loss: 2.2407 - val_accuracy: 0.9338\n",
            "Epoch 377/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.7179 - accuracy: 0.9429 - val_loss: 2.7196 - val_accuracy: 0.9288\n",
            "Epoch 378/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.8896 - accuracy: 0.9417 - val_loss: 2.6611 - val_accuracy: 0.9326\n",
            "Epoch 379/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.1054 - accuracy: 0.9283 - val_loss: 2.0834 - val_accuracy: 0.9389\n",
            "Epoch 380/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.6360 - accuracy: 0.9407 - val_loss: 1.7354 - val_accuracy: 0.9440\n",
            "Epoch 381/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.7386 - accuracy: 0.9445 - val_loss: 1.9172 - val_accuracy: 0.9427\n",
            "Epoch 382/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.6910 - accuracy: 0.9442 - val_loss: 2.5407 - val_accuracy: 0.9338\n",
            "Epoch 383/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.2365 - accuracy: 0.9340 - val_loss: 2.0205 - val_accuracy: 0.9338\n",
            "Epoch 384/500\n",
            "99/98 [==============================] - 34s 338ms/step - loss: 3.1589 - accuracy: 0.9222 - val_loss: 3.0580 - val_accuracy: 0.9237\n",
            "Epoch 385/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.2164 - accuracy: 0.9378 - val_loss: 1.8835 - val_accuracy: 0.9389\n",
            "Epoch 386/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.7903 - accuracy: 0.9426 - val_loss: 1.7785 - val_accuracy: 0.9440\n",
            "Epoch 387/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1727 - accuracy: 0.9331 - val_loss: 2.2113 - val_accuracy: 0.9288\n",
            "Epoch 388/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.5948 - accuracy: 0.9347 - val_loss: 1.9407 - val_accuracy: 0.9402\n",
            "Epoch 389/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0861 - accuracy: 0.9359 - val_loss: 1.8042 - val_accuracy: 0.9453\n",
            "Epoch 390/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0175 - accuracy: 0.9420 - val_loss: 2.5801 - val_accuracy: 0.9338\n",
            "Epoch 391/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.9756 - accuracy: 0.9254 - val_loss: 1.9993 - val_accuracy: 0.9453\n",
            "Epoch 392/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.5507 - accuracy: 0.9280 - val_loss: 1.8627 - val_accuracy: 0.9402\n",
            "Epoch 393/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.1204 - accuracy: 0.9404 - val_loss: 2.0676 - val_accuracy: 0.9389\n",
            "Epoch 394/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8581 - accuracy: 0.9429 - val_loss: 2.6261 - val_accuracy: 0.9313\n",
            "Epoch 395/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.1071 - accuracy: 0.9429 - val_loss: 1.9169 - val_accuracy: 0.9338\n",
            "Epoch 396/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.2507 - accuracy: 0.9356 - val_loss: 1.7063 - val_accuracy: 0.9440\n",
            "Epoch 397/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.1871 - accuracy: 0.9369 - val_loss: 1.6197 - val_accuracy: 0.9504\n",
            "Epoch 398/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0464 - accuracy: 0.9356 - val_loss: 2.5603 - val_accuracy: 0.9300\n",
            "Epoch 399/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8050 - accuracy: 0.9461 - val_loss: 2.0274 - val_accuracy: 0.9300\n",
            "Epoch 400/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9590 - accuracy: 0.9382 - val_loss: 2.2760 - val_accuracy: 0.9415\n",
            "Epoch 401/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.5828 - accuracy: 0.9458 - val_loss: 2.1581 - val_accuracy: 0.9389\n",
            "Epoch 402/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.0478 - accuracy: 0.9343 - val_loss: 1.8843 - val_accuracy: 0.9478\n",
            "Epoch 403/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9598 - accuracy: 0.9366 - val_loss: 1.7106 - val_accuracy: 0.9491\n",
            "Epoch 404/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.2747 - accuracy: 0.9343 - val_loss: 1.7282 - val_accuracy: 0.9440\n",
            "Epoch 405/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.7268 - accuracy: 0.9464 - val_loss: 1.4567 - val_accuracy: 0.9491\n",
            "Epoch 406/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.7424 - accuracy: 0.9308 - val_loss: 1.9942 - val_accuracy: 0.9440\n",
            "Epoch 407/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.2015 - accuracy: 0.9394 - val_loss: 1.6754 - val_accuracy: 0.9440\n",
            "Epoch 408/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.6618 - accuracy: 0.9468 - val_loss: 1.6176 - val_accuracy: 0.9453\n",
            "Epoch 409/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.6902 - accuracy: 0.9436 - val_loss: 1.5152 - val_accuracy: 0.9517\n",
            "Epoch 410/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.7865 - accuracy: 0.9442 - val_loss: 2.0716 - val_accuracy: 0.9402\n",
            "Epoch 411/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 1.9811 - accuracy: 0.9464 - val_loss: 2.0951 - val_accuracy: 0.9326\n",
            "Epoch 412/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.0079 - accuracy: 0.9391 - val_loss: 1.8702 - val_accuracy: 0.9364\n",
            "Epoch 413/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.9813 - accuracy: 0.9410 - val_loss: 1.5847 - val_accuracy: 0.9466\n",
            "Epoch 414/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.9755 - accuracy: 0.9420 - val_loss: 1.5296 - val_accuracy: 0.9478\n",
            "Epoch 415/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8831 - accuracy: 0.9426 - val_loss: 1.6931 - val_accuracy: 0.9491\n",
            "Epoch 416/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9062 - accuracy: 0.9442 - val_loss: 2.5977 - val_accuracy: 0.9351\n",
            "Epoch 417/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0597 - accuracy: 0.9366 - val_loss: 2.3743 - val_accuracy: 0.9402\n",
            "Epoch 418/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.9788 - accuracy: 0.9398 - val_loss: 1.8999 - val_accuracy: 0.9338\n",
            "Epoch 419/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.3904 - accuracy: 0.9327 - val_loss: 3.1431 - val_accuracy: 0.9097\n",
            "Epoch 420/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.2116 - accuracy: 0.9401 - val_loss: 1.7496 - val_accuracy: 0.9504\n",
            "Epoch 421/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8889 - accuracy: 0.9436 - val_loss: 1.7775 - val_accuracy: 0.9517\n",
            "Epoch 422/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.7834 - accuracy: 0.9433 - val_loss: 1.6484 - val_accuracy: 0.9491\n",
            "Epoch 423/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.9310 - accuracy: 0.9429 - val_loss: 1.6727 - val_accuracy: 0.9466\n",
            "Epoch 424/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8584 - accuracy: 0.9429 - val_loss: 1.5145 - val_accuracy: 0.9491\n",
            "Epoch 425/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.2621 - accuracy: 0.9452 - val_loss: 1.6001 - val_accuracy: 0.9555\n",
            "Epoch 426/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.1108 - accuracy: 0.9417 - val_loss: 1.7944 - val_accuracy: 0.9466\n",
            "Epoch 427/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.7934 - accuracy: 0.9413 - val_loss: 1.9044 - val_accuracy: 0.9478\n",
            "Epoch 428/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.4868 - accuracy: 0.9315 - val_loss: 1.9294 - val_accuracy: 0.9466\n",
            "Epoch 429/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1302 - accuracy: 0.9436 - val_loss: 1.6548 - val_accuracy: 0.9453\n",
            "Epoch 430/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.8464 - accuracy: 0.9439 - val_loss: 1.7686 - val_accuracy: 0.9338\n",
            "Epoch 431/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0595 - accuracy: 0.9388 - val_loss: 1.6603 - val_accuracy: 0.9517\n",
            "Epoch 432/500\n",
            "99/98 [==============================] - 34s 338ms/step - loss: 2.1267 - accuracy: 0.9356 - val_loss: 1.8756 - val_accuracy: 0.9478\n",
            "Epoch 433/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0046 - accuracy: 0.9468 - val_loss: 1.4246 - val_accuracy: 0.9491\n",
            "Epoch 434/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0428 - accuracy: 0.9350 - val_loss: 1.5978 - val_accuracy: 0.9542\n",
            "Epoch 435/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.8333 - accuracy: 0.9433 - val_loss: 1.5989 - val_accuracy: 0.9453\n",
            "Epoch 436/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0258 - accuracy: 0.9429 - val_loss: 1.4119 - val_accuracy: 0.9529\n",
            "Epoch 437/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.8561 - accuracy: 0.9461 - val_loss: 1.3873 - val_accuracy: 0.9529\n",
            "Epoch 438/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.2226 - accuracy: 0.9321 - val_loss: 2.6211 - val_accuracy: 0.9313\n",
            "Epoch 439/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.9283 - accuracy: 0.9426 - val_loss: 2.3111 - val_accuracy: 0.9249\n",
            "Epoch 440/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.2362 - accuracy: 0.9362 - val_loss: 2.1775 - val_accuracy: 0.9377\n",
            "Epoch 441/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.3662 - accuracy: 0.9407 - val_loss: 1.5769 - val_accuracy: 0.9555\n",
            "Epoch 442/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9975 - accuracy: 0.9385 - val_loss: 1.6126 - val_accuracy: 0.9453\n",
            "Epoch 443/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.5668 - accuracy: 0.9480 - val_loss: 2.6005 - val_accuracy: 0.9275\n",
            "Epoch 444/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.1548 - accuracy: 0.9356 - val_loss: 1.6865 - val_accuracy: 0.9491\n",
            "Epoch 445/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9320 - accuracy: 0.9423 - val_loss: 1.9539 - val_accuracy: 0.9491\n",
            "Epoch 446/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.0589 - accuracy: 0.9388 - val_loss: 2.4556 - val_accuracy: 0.9313\n",
            "Epoch 447/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.1270 - accuracy: 0.9378 - val_loss: 1.9959 - val_accuracy: 0.9453\n",
            "Epoch 448/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.0200 - accuracy: 0.9429 - val_loss: 2.3010 - val_accuracy: 0.9338\n",
            "Epoch 449/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.9058 - accuracy: 0.9391 - val_loss: 1.7672 - val_accuracy: 0.9402\n",
            "Epoch 450/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.2042 - accuracy: 0.9385 - val_loss: 2.3423 - val_accuracy: 0.9300\n",
            "Epoch 451/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 1.9301 - accuracy: 0.9426 - val_loss: 1.7372 - val_accuracy: 0.9415\n",
            "Epoch 452/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.8594 - accuracy: 0.9452 - val_loss: 1.5721 - val_accuracy: 0.9517\n",
            "Epoch 453/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.1046 - accuracy: 0.9436 - val_loss: 1.5981 - val_accuracy: 0.9415\n",
            "Epoch 454/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0052 - accuracy: 0.9378 - val_loss: 1.7052 - val_accuracy: 0.9427\n",
            "Epoch 455/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.1961 - accuracy: 0.9356 - val_loss: 1.5776 - val_accuracy: 0.9529\n",
            "Epoch 456/500\n",
            "99/98 [==============================] - 33s 333ms/step - loss: 2.0250 - accuracy: 0.9413 - val_loss: 1.4026 - val_accuracy: 0.9593\n",
            "Epoch 457/500\n",
            "99/98 [==============================] - 33s 333ms/step - loss: 1.9913 - accuracy: 0.9366 - val_loss: 1.7273 - val_accuracy: 0.9402\n",
            "Epoch 458/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 1.9583 - accuracy: 0.9417 - val_loss: 1.5889 - val_accuracy: 0.9517\n",
            "Epoch 459/500\n",
            "99/98 [==============================] - 33s 333ms/step - loss: 1.9707 - accuracy: 0.9500 - val_loss: 1.4251 - val_accuracy: 0.9580\n",
            "Epoch 460/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.2715 - accuracy: 0.9407 - val_loss: 2.3197 - val_accuracy: 0.9237\n",
            "Epoch 461/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.3583 - accuracy: 0.9353 - val_loss: 5.1331 - val_accuracy: 0.8969\n",
            "Epoch 462/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.0721 - accuracy: 0.9452 - val_loss: 2.1113 - val_accuracy: 0.9402\n",
            "Epoch 463/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0835 - accuracy: 0.9385 - val_loss: 2.3371 - val_accuracy: 0.9300\n",
            "Epoch 464/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9890 - accuracy: 0.9423 - val_loss: 2.9181 - val_accuracy: 0.9288\n",
            "Epoch 465/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.6784 - accuracy: 0.9401 - val_loss: 1.6110 - val_accuracy: 0.9491\n",
            "Epoch 466/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 1.8328 - accuracy: 0.9353 - val_loss: 3.4910 - val_accuracy: 0.9198\n",
            "Epoch 467/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.8818 - accuracy: 0.9420 - val_loss: 1.7490 - val_accuracy: 0.9504\n",
            "Epoch 468/500\n",
            "99/98 [==============================] - 33s 335ms/step - loss: 2.1019 - accuracy: 0.9394 - val_loss: 2.8468 - val_accuracy: 0.9275\n",
            "Epoch 469/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.1105 - accuracy: 0.9375 - val_loss: 2.1910 - val_accuracy: 0.9262\n",
            "Epoch 470/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 1.8204 - accuracy: 0.9420 - val_loss: 1.6299 - val_accuracy: 0.9555\n",
            "Epoch 471/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 2.0226 - accuracy: 0.9464 - val_loss: 2.3962 - val_accuracy: 0.9351\n",
            "Epoch 472/500\n",
            "99/98 [==============================] - 33s 333ms/step - loss: 2.1933 - accuracy: 0.9362 - val_loss: 2.3867 - val_accuracy: 0.9338\n",
            "Epoch 473/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 1.9109 - accuracy: 0.9426 - val_loss: 4.4889 - val_accuracy: 0.9084\n",
            "Epoch 474/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 2.1776 - accuracy: 0.9372 - val_loss: 2.8207 - val_accuracy: 0.9300\n",
            "Epoch 475/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1002 - accuracy: 0.9382 - val_loss: 1.6362 - val_accuracy: 0.9517\n",
            "Epoch 476/500\n",
            "99/98 [==============================] - 33s 336ms/step - loss: 1.8677 - accuracy: 0.9439 - val_loss: 1.5753 - val_accuracy: 0.9478\n",
            "Epoch 477/500\n",
            "99/98 [==============================] - 33s 334ms/step - loss: 2.1041 - accuracy: 0.9410 - val_loss: 1.7040 - val_accuracy: 0.9504\n",
            "Epoch 478/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 2.0372 - accuracy: 0.9378 - val_loss: 1.6498 - val_accuracy: 0.9529\n",
            "Epoch 479/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 1.8356 - accuracy: 0.9410 - val_loss: 2.3938 - val_accuracy: 0.9338\n",
            "Epoch 480/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 1.9855 - accuracy: 0.9398 - val_loss: 1.6452 - val_accuracy: 0.9555\n",
            "Epoch 481/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.0556 - accuracy: 0.9500 - val_loss: 3.7755 - val_accuracy: 0.8931\n",
            "Epoch 482/500\n",
            "99/98 [==============================] - 33s 337ms/step - loss: 1.9242 - accuracy: 0.9385 - val_loss: 2.1606 - val_accuracy: 0.9389\n",
            "Epoch 483/500\n",
            "99/98 [==============================] - 34s 339ms/step - loss: 2.2344 - accuracy: 0.9372 - val_loss: 3.0363 - val_accuracy: 0.9186\n",
            "Epoch 484/500\n",
            "99/98 [==============================] - 33s 338ms/step - loss: 2.1294 - accuracy: 0.9429 - val_loss: 1.7343 - val_accuracy: 0.9453\n",
            "Epoch 485/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.0126 - accuracy: 0.9378 - val_loss: 2.0358 - val_accuracy: 0.9402\n",
            "Epoch 486/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 1.9076 - accuracy: 0.9420 - val_loss: 2.0765 - val_accuracy: 0.9377\n",
            "Epoch 487/500\n",
            "99/98 [==============================] - 34s 343ms/step - loss: 1.8102 - accuracy: 0.9500 - val_loss: 1.4648 - val_accuracy: 0.9542\n",
            "Epoch 488/500\n",
            "99/98 [==============================] - 34s 343ms/step - loss: 2.3756 - accuracy: 0.9350 - val_loss: 1.8344 - val_accuracy: 0.9491\n",
            "Epoch 489/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 2.2432 - accuracy: 0.9353 - val_loss: 1.9054 - val_accuracy: 0.9402\n",
            "Epoch 490/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 3.0494 - accuracy: 0.9296 - val_loss: 2.0840 - val_accuracy: 0.9466\n",
            "Epoch 491/500\n",
            "99/98 [==============================] - 34s 340ms/step - loss: 2.4909 - accuracy: 0.9391 - val_loss: 1.8649 - val_accuracy: 0.9478\n",
            "Epoch 492/500\n",
            "99/98 [==============================] - 34s 343ms/step - loss: 2.0794 - accuracy: 0.9442 - val_loss: 2.4109 - val_accuracy: 0.9389\n",
            "Epoch 493/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.9574 - accuracy: 0.9413 - val_loss: 2.3647 - val_accuracy: 0.9402\n",
            "Epoch 494/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.9594 - accuracy: 0.9413 - val_loss: 4.0736 - val_accuracy: 0.9097\n",
            "Epoch 495/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 2.1172 - accuracy: 0.9401 - val_loss: 1.9475 - val_accuracy: 0.9453\n",
            "Epoch 496/500\n",
            "99/98 [==============================] - 34s 344ms/step - loss: 1.9231 - accuracy: 0.9401 - val_loss: 1.9462 - val_accuracy: 0.9427\n",
            "Epoch 497/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 2.1173 - accuracy: 0.9388 - val_loss: 1.9442 - val_accuracy: 0.9466\n",
            "Epoch 498/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 2.1923 - accuracy: 0.9426 - val_loss: 2.0274 - val_accuracy: 0.9427\n",
            "Epoch 499/500\n",
            "99/98 [==============================] - 34s 342ms/step - loss: 1.9781 - accuracy: 0.9359 - val_loss: 3.1804 - val_accuracy: 0.9122\n",
            "Epoch 500/500\n",
            "99/98 [==============================] - 34s 341ms/step - loss: 2.1948 - accuracy: 0.9350 - val_loss: 1.9364 - val_accuracy: 0.9529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48WerEQHda3P"
      },
      "source": [
        "model.save('inceptionchest_00matr.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGliPxSb6umH"
      },
      "source": [
        "model.save('inceptionchest_00.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XOd_jvDffO7"
      },
      "source": [
        "model.save_weights('inceptionv3chest_00matr.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yml61NbX6wE2"
      },
      "source": [
        "model.save_weights('inceptionv3chest_00.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfnsaxOjfiyh"
      },
      "source": [
        "model = load_model('inceptionchest_00matr.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BgAVnVMRhrM"
      },
      "source": [
        "path='Test/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phWro75kSBI2"
      },
      "source": [
        "### **Making Predicions of Unseen data (covid total=1161,non covid total=250)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE50gLvzV3wB"
      },
      "source": [
        "#after prediction \n",
        "**94% acc in train and 95% val(model-1) gives 1379 correct result and 21 wrong in unseen data**\n",
        "\n",
        "**94% acc in train and val (model-2) gives 1352 correct result and 46 wrong in unseen data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dso993Mv7sEF",
        "outputId": "9dd59b90-d904-41f0-f46f-4115076f5e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "c=0\n",
        "o=0\n",
        "for x in os.listdir(path):\n",
        "  #print(x)\n",
        "  image = cv2.imread(path+x) # read file \n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # arrange format as per keras\n",
        "  image = cv2.resize(image,(224,224))\n",
        "  image = np.array(image) / 255\n",
        "  image = np.expand_dims(image, axis=0)\n",
        "  \n",
        "  y_pred = model.predict(image)\n",
        "  probability = y_pred[0]\n",
        "  if probability[0] > 0.5:\n",
        "    c=c+1\n",
        "    vgg_chest_pred = str(' COVID') \n",
        "    \n",
        "  else:\n",
        "    o=o+1\n",
        "    vgg_chest_pred = str('NonCOVID')\n",
        "print(c)\n",
        "print(o)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-36825e302459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRMVabiJU2V4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBiA8iqGSGr7"
      },
      "source": [
        "### Check on my data predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S22ulPybSYLe"
      },
      "source": [
        "probability = y_pred[0]\n",
        "print(\"VGG Predictions:\")\n",
        "if probability[0] > 0.5:\n",
        "  vgg_chest_pred = str(' COVID') \n",
        "else:\n",
        "  vgg_chest_pred = str('NonCOVID')\n",
        "print(vgg_chest_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eckDLREO3lJw"
      },
      "source": [
        "y_pred = model.predict(X_test, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7OnPLZZp3tq"
      },
      "source": [
        "# Convert to Binary classes\n",
        "y_pred_bin = np.argmax(y_pred, axis=1)\n",
        "y_test_bin = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAPfnQ2Np3PE"
      },
      "source": [
        "### Plot ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7k3UOqThPLw",
        "outputId": "a0819406-c675-4eda-c356-de607e8b0062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred_bin)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.title('ROC curve for our model')\n",
        "plt.xlabel('False Positive Rate/Sensitivity')\n",
        "plt.ylabel('True Positive Rate/specificity')\n",
        "plt.savefig('inception_roc.png')\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNyZRJ1JfYC1"
      },
      "source": [
        "### Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t80JK23kVxua",
        "outputId": "76a44982-e7d3-4607-9c3d-92e234a0001e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "def plot_confusion_matrix(normalize):\n",
        "  classes = ['COVID','NonCOVID']\n",
        "  tick_marks = [0.5,1.5]\n",
        "  cn = confusion_matrix(y_test_bin, y_pred_bin,normalize=normalize)\n",
        "  sns.heatmap(cn,cmap='plasma',annot=True)\n",
        "  plt.xticks(tick_marks, classes)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.ylabel('Predicted label')\n",
        "  plt.xlabel('Actual label')\n",
        "  plt.savefig('inception_cm.png')\n",
        "  plt.show()\n",
        "\n",
        "print('Confusion Matrix without Normalization')\n",
        "plot_confusion_matrix(normalize=None)\n",
        "\n",
        "print('Confusion Matrix with Normalized Values')\n",
        "plot_confusion_matrix(normalize='true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix without Normalization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix with Normalized Values\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZoH_wCSMDD4"
      },
      "source": [
        "### **Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFSCY2HH7uVc",
        "outputId": "7fd6d9be-fb56-4f08-ffe2-dddf06271d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_bin, y_pred_bin))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.98      0.95       396\n",
            "           1       0.98      0.93      0.95       390\n",
            "\n",
            "    accuracy                           0.95       786\n",
            "   macro avg       0.95      0.95      0.95       786\n",
            "weighted avg       0.95      0.95      0.95       786\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1makQs5MMlU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMlrl1tl9bDY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}